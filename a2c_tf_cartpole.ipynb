{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v0\"\n",
    "num_envs = 3\n",
    "n_steps = 50\n",
    "n_actions = 2\n",
    "state_size = 4\n",
    "entropy_coef = 0\n",
    "vf_coef = 0.1\n",
    "learning_rate = 0.002\n",
    "lam = 0.95\n",
    "gamma = 0.99\n",
    "episodes = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing_env import SubprocVecEnv\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "X = tf.placeholder(tf.float32, shape=(None, state_size), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None, n_actions), name=\"y\")\n",
    "advantages = tf.placeholder(tf.float32, [None], name=\"advantages\")\n",
    "rewards = tf.placeholder(tf.float32, [None], name=\"rewards\")\n",
    "\n",
    "# Hidden\n",
    "fc1 = tf.layers.dense(X, 32, activation=tf.nn.relu, kernel_initializer=\n",
    "tf.contrib.layers.xavier_initializer())\n",
    "fc2 = tf.layers.dense(fc1, 64, activation=tf.nn.relu, kernel_initializer=\n",
    "tf.contrib.layers.xavier_initializer())\n",
    "fc3 = tf.layers.dense(fc1, 10, activation=tf.nn.relu, kernel_initializer=\n",
    "tf.contrib.layers.xavier_initializer())\n",
    "fc4 = tf.layers.dense(fc3, n_actions, activation=None)\n",
    "# Output\n",
    "action_distribution = tf.nn.softmax(fc4)\n",
    "value = tf.layers.dense(fc3, 1, activation=None, kernel_initializer=\n",
    "tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "# Loss\n",
    "neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=fc4, labels=y)\n",
    "pg_loss = tf.reduce_mean(advantages * neg_log_prob)\n",
    "vf_loss = tf.losses.mean_squared_error(tf.squeeze(value), rewards)\n",
    "\n",
    "entropy = 0 # TODO: Implement entropy\n",
    "loss = pg_loss - entropy * entropy_coef + vf_loss * vf_coef\n",
    "\n",
    "# Gradient Clipping\n",
    "params = tf.trainable_variables()\n",
    "\n",
    "grads = tf.gradients(loss, params)\n",
    "grads, grad_norm = tf.clip_by_global_norm(grads, 0.5)\n",
    "grads = list(zip(grads, params))\n",
    "\n",
    "# Training\n",
    "trainer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train = trainer.apply_gradients(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(\"./tensorboard/5\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_mean = tf.placeholder(tf.float32, name=\"reward_mean\")\n",
    "policy_loss = tf.placeholder(tf.float32, name=\"policy_loss\")\n",
    "value_loss = tf.placeholder(tf.float32, name=\"value_loss\")\n",
    "tf.summary.scalar(\"Reward_mean\", reward_mean)\n",
    "tf.summary.scalar(\"Policy_loss\", policy_loss)\n",
    "tf.summary.scalar(\"Value_loss\", value_loss)\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discounted sum of rewards "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_with_dones(next_value, rewards, masks, gamma):\n",
    "    r = next_value.reshape(next_value.shape[0])\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        r = rewards[step] + gamma * r * masks[step]\n",
    "        returns.insert(0, r)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = envs.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(states.shape)\n",
    "with tf.Session() as sess:\n",
    "    reward_mean_ = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for ep in range(episodes):\n",
    "        mb_states, mb_actions, mb_rewards, mb_values, mb_dones, mb_masks = [],[],[],[],[], []\n",
    "        for _ in range(n_steps):\n",
    "            # Feed forward current states \n",
    "            action_dist, values = sess.run([action_distribution, value], {X:states})\n",
    "            # Sample actions from action_dist\n",
    "            action = list(map(lambda p: np.random.choice(range(2), p=p), action_dist))\n",
    "            mb_states.append(states)\n",
    "            \n",
    "            states, reward, done, _ = envs.step(action)\n",
    "\n",
    "            n_values = np.max(n_actions)\n",
    "            mb_actions.append(np.eye(n_values)[action])\n",
    "\n",
    "            mb_values.append(values.flatten())\n",
    "            mb_dones.append(done)\n",
    "            mb_rewards.append(reward)\n",
    "            mb_masks.append(1-done)\n",
    "        next_values = sess.run(value, {X: states})\n",
    "        mb_rewards = np.asarray(mb_rewards)\n",
    "        mb_returns = discount_with_dones(next_values, mb_rewards, mb_masks, gamma)\n",
    "        mb_returns = np.asarray(mb_returns)\n",
    "\n",
    "        mb_actions = np.asarray(mb_actions)\n",
    "        mb_states = np.asarray(mb_states)\n",
    "        mb_values = np.asarray(mb_values)\n",
    "\n",
    "        \n",
    "\n",
    "        mb_advs = np.asarray(mb_returns) - np.asarray(mb_values)\n",
    "        mb_states = mb_states.reshape(mb_states.shape[0]*mb_states.shape[1],mb_states.shape[2])\n",
    "        mb_advs = mb_advs.reshape(mb_advs.shape[0]*mb_advs.shape[1])\n",
    "        mb_returns = mb_returns.reshape(mb_returns.shape[0]*mb_returns.shape[1])\n",
    "        mb_actions = mb_actions.reshape(mb_actions.shape[0]*mb_actions.shape[1],mb_actions.shape[2])\n",
    "        \n",
    "        feed_dict = {X: mb_states,\n",
    "            y: mb_actions,\n",
    "            advantages: mb_advs, # Use to calculate our policy loss\n",
    "            rewards: mb_returns}\n",
    "\n",
    "        policy_loss_, value_loss_, loss_, _ = sess.run([pg_loss, vf_loss, loss, train], feed_dict)\n",
    "        print(\"Value:\",value_loss_)\n",
    "        print(\"Policy:\",policy_loss_)\n",
    "        print(\"Loss:\",loss_)\n",
    "        \n",
    "        if(ep % 10 == 0):\n",
    "            tstate = env.reset()\n",
    "            done = False\n",
    "            rewards_ = []\n",
    "            while not done:\n",
    "                #env.render()\n",
    "                action_dist, values = sess.run([action_distribution, value], {X:tstate.reshape(1,tstate.shape[0])})\n",
    "                action=np.random.choice(range(2), p=action_dist[0])\n",
    "                tstate, reward, done, _ = env.step(action)\n",
    "                rewards_.append(reward)\n",
    "            reward_mean_ = np.sum(rewards_)\n",
    "        summary = sess.run(write_op, feed_dict={reward_mean: reward_mean_,policy_loss: policy_loss_, value_loss: value_loss_})\n",
    "        writer.add_summary(summary, ep)\n",
    "        writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
