{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/f.seiler/anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env = env.unwrapped\n",
    "# Policy gradient has high variance, seed for reproducability\n",
    "env.seed(1)\n",
    "\n",
    "## ENVIRONMENT Hyperparameters\n",
    "state_size = 2\n",
    "action_size = env.action_space.n\n",
    "\n",
    "## TRAINING Hyperparameters\n",
    "max_episodes = 1000\n",
    "learning_rate = 0.01\n",
    "gamma = 0.95 # Discount rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discounted Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(episode_rewards):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "    \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "    \n",
    "    return discounted_episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/f.seiler/anaconda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    with tf.name_scope(\"inputs\"):\n",
    "        input_ = tf.placeholder(tf.float32, [None, state_size], name=\"input_\")\n",
    "        actions = tf.placeholder(tf.int32, [None, action_size], name=\"actions\")\n",
    "        discounted_episode_rewards_ = tf.placeholder(tf.float32, [None,], name=\"discounted_episode_rewards\")\n",
    "\n",
    "        # Add this placeholder for having this variable in tensorboard\n",
    "        mean_reward_ = tf.placeholder(tf.float32 , name=\"mean_reward\")\n",
    "        avg_loss = tf.placeholder(tf.float32, name=\"avg_loss\")\n",
    "        \n",
    "        with tf.name_scope(\"fc1\"):\n",
    "            fc1 = tf.contrib.layers.fully_connected(inputs = input_,\n",
    "                                                    num_outputs = 30, weights_regularizer=tf.contrib.layers.l2_regularizer(0.001),\n",
    "                                                    activation_fn=tf.nn.relu,\n",
    "                                                    weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        with tf.name_scope(\"fc2\"):\n",
    "            fc2 = tf.contrib.layers.fully_connected(inputs = fc1,\n",
    "                                                    num_outputs = 10,  weights_regularizer=tf.contrib.layers.l2_regularizer(0.001),\n",
    "                                                    activation_fn= tf.nn.relu,\n",
    "                                                    weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        with tf.name_scope(\"fc3\"):\n",
    "            fc3 = tf.contrib.layers.fully_connected(inputs = fc2,\n",
    "                                                    num_outputs = action_size,  weights_regularizer=tf.contrib.layers.l2_regularizer(0.001),\n",
    "                                                    activation_fn= None,\n",
    "                                                    weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        with tf.name_scope(\"softmax\"):\n",
    "            action_distribution = tf.nn.softmax(fc3)\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            # tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function\n",
    "            # If you have single-class labels, where an object can only belong to one class, you might now consider using \n",
    "            # tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array.\n",
    "            #time.sleep(4)\n",
    "            reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "            reg_constant = 0.01  # Choose an appropriate one.\n",
    "            neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits = fc3, labels = actions)\n",
    "            loss = tf.reduce_mean(neg_log_prob * discounted_episode_rewards_) + reg_constant * sum(reg_losses)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"train\"):\n",
    "            train_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Setup Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "with g.as_default():\n",
    "    writer = tf.summary.FileWriter(\"./tensorboard/pg/2\", g)\n",
    "\n",
    "    ## Losses\n",
    "    tf.summary.scalar(\"Loss\", loss)\n",
    "\n",
    "    ## Loss / Avg\n",
    "    tf.summary.scalar(\"Average-Loss\", avg_loss)\n",
    "    \n",
    "    ## Reward mean\n",
    "    tf.summary.scalar(\"Reward_mean\", mean_reward_)\n",
    "\n",
    "    write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  0\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "-0.00057542406\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  1\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "0.001961532\n",
      "==========================================\n",
      "Episode:  2\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "0.00092870515\n",
      "==========================================\n",
      "Episode:  3\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "0.002548294\n",
      "==========================================\n",
      "Episode:  4\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "-0.0020290152\n",
      "==========================================\n",
      "Episode:  5\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "0.0005786442\n",
      "==========================================\n",
      "Episode:  6\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "0.0018993485\n",
      "==========================================\n",
      "Episode:  7\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "-0.00017980233\n",
      "==========================================\n",
      "Episode:  8\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "-0.0007117698\n",
      "==========================================\n",
      "Episode:  9\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "-4.328962e-05\n",
      "==========================================\n",
      "Episode:  10\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "-0.00049152266\n",
      "==========================================\n",
      "Episode:  11\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "-0.00037663264\n",
      "==========================================\n",
      "Episode:  12\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "-0.0054720296\n",
      "==========================================\n",
      "Episode:  13\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "0.0019401074\n",
      "==========================================\n",
      "Episode:  14\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "-0.003735601\n",
      "==========================================\n",
      "Episode:  15\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "-0.0022223545\n",
      "==========================================\n",
      "Episode:  16\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "0.0016394001\n",
      "==========================================\n",
      "Episode:  17\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "-0.0060105287\n",
      "==========================================\n",
      "Episode:  18\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "0.0021988924\n",
      "==========================================\n",
      "Episode:  19\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "0.0037693852\n",
      "==========================================\n",
      "Episode:  20\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "-0.0039920458\n",
      "==========================================\n",
      "Episode:  21\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "-0.00031171838\n",
      "==========================================\n",
      "Episode:  22\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "0.00327394\n",
      "==========================================\n",
      "Episode:  23\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "-0.013107546\n",
      "==========================================\n",
      "Episode:  24\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "0.0067761573\n",
      "==========================================\n",
      "Episode:  25\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "-0.023772502\n",
      "==========================================\n",
      "Episode:  26\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2500.0\n",
      "Max reward so far:  -2500.0\n",
      "0.008970022\n",
      "==========================================\n",
      "Episode:  27\n",
      "Reward:  -2175.0\n",
      "Mean Reward -2488.3928571428573\n",
      "Max reward so far:  -2175.0\n",
      "0.009170488\n",
      "==========================================\n",
      "Episode:  28\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2488.793103448276\n",
      "Max reward so far:  -2175.0\n",
      "0.010056606\n",
      "==========================================\n",
      "Episode:  29\n",
      "Reward:  -2044.0\n",
      "Mean Reward -2473.9666666666667\n",
      "Max reward so far:  -2044.0\n",
      "0.015259991\n",
      "==========================================\n",
      "Episode:  30\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2474.8064516129034\n",
      "Max reward so far:  -2044.0\n",
      "-0.008742571\n",
      "==========================================\n",
      "Episode:  31\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2475.59375\n",
      "Max reward so far:  -2044.0\n",
      "-0.015258445\n",
      "==========================================\n",
      "Episode:  32\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2476.3333333333335\n",
      "Max reward so far:  -2044.0\n",
      "0.005574656\n",
      "==========================================\n",
      "Episode:  33\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2477.029411764706\n",
      "Max reward so far:  -2044.0\n",
      "0.011485832\n",
      "==========================================\n",
      "Episode:  34\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2477.6857142857143\n",
      "Max reward so far:  -2044.0\n",
      "-0.0030582373\n",
      "==========================================\n",
      "Episode:  35\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2478.3055555555557\n",
      "Max reward so far:  -2044.0\n",
      "0.015707944\n",
      "==========================================\n",
      "Episode:  36\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2478.891891891892\n",
      "Max reward so far:  -2044.0\n",
      "-0.007994171\n",
      "==========================================\n",
      "Episode:  37\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2479.4473684210525\n",
      "Max reward so far:  -2044.0\n",
      "0.011620416\n",
      "==========================================\n",
      "Episode:  38\n",
      "Reward:  -1542.0\n",
      "Mean Reward -2455.4102564102564\n",
      "Max reward so far:  -1542.0\n",
      "0.019712392\n",
      "==========================================\n",
      "Episode:  39\n",
      "Reward:  -2148.0\n",
      "Mean Reward -2447.725\n",
      "Max reward so far:  -1542.0\n",
      "0.008763121\n",
      "==========================================\n",
      "Episode:  40\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2449.0\n",
      "Max reward so far:  -1542.0\n",
      "0.018587349\n",
      "==========================================\n",
      "Episode:  41\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2450.214285714286\n",
      "Max reward so far:  -1542.0\n",
      "-0.0049446453\n",
      "==========================================\n",
      "Episode:  42\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2451.3720930232557\n",
      "Max reward so far:  -1542.0\n",
      "-0.004040671\n",
      "==========================================\n",
      "Episode:  43\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2452.4772727272725\n",
      "Max reward so far:  -1542.0\n",
      "0.00502473\n",
      "==========================================\n",
      "Episode:  44\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2453.5333333333333\n",
      "Max reward so far:  -1542.0\n",
      "-0.008534141\n",
      "==========================================\n",
      "Episode:  45\n",
      "Reward:  -2284.0\n",
      "Mean Reward -2449.8478260869565\n",
      "Max reward so far:  -1542.0\n",
      "0.003118853\n",
      "==========================================\n",
      "Episode:  46\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2450.9148936170213\n",
      "Max reward so far:  -1542.0\n",
      "0.016788755\n",
      "==========================================\n",
      "Episode:  47\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2451.9375\n",
      "Max reward so far:  -1542.0\n",
      "-0.003311514\n",
      "==========================================\n",
      "Episode:  48\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2452.918367346939\n",
      "Max reward so far:  -1542.0\n",
      "-0.001657892\n",
      "==========================================\n",
      "Episode:  49\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2453.86\n",
      "Max reward so far:  -1542.0\n",
      "-0.0120399175\n",
      "==========================================\n",
      "Episode:  50\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2454.764705882353\n",
      "Max reward so far:  -1542.0\n",
      "-0.00070092233\n",
      "==========================================\n",
      "Episode:  51\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2455.6346153846152\n",
      "Max reward so far:  -1542.0\n",
      "-0.012988832\n",
      "==========================================\n",
      "Episode:  52\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2456.4716981132074\n",
      "Max reward so far:  -1542.0\n",
      "-0.0043012477\n",
      "==========================================\n",
      "Episode:  53\n",
      "Reward:  -2330.0\n",
      "Mean Reward -2454.1296296296296\n",
      "Max reward so far:  -1542.0\n",
      "0.0033375106\n",
      "==========================================\n",
      "Episode:  54\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2454.9636363636364\n",
      "Max reward so far:  -1542.0\n",
      "-0.00236227\n",
      "==========================================\n",
      "Episode:  55\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2455.7678571428573\n",
      "Max reward so far:  -1542.0\n",
      "-0.013389406\n",
      "==========================================\n",
      "Episode:  56\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2456.5438596491226\n",
      "Max reward so far:  -1542.0\n",
      "-0.010941263\n",
      "==========================================\n",
      "Episode:  57\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2457.293103448276\n",
      "Max reward so far:  -1542.0\n",
      "-0.014162766\n",
      "==========================================\n",
      "Episode:  58\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2458.0169491525426\n",
      "Max reward so far:  -1542.0\n",
      "-0.0035426896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  59\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2458.7166666666667\n",
      "Max reward so far:  -1542.0\n",
      "-0.023305612\n",
      "==========================================\n",
      "Episode:  60\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2459.3934426229507\n",
      "Max reward so far:  -1542.0\n",
      "-0.0051098596\n",
      "==========================================\n",
      "Episode:  61\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2460.048387096774\n",
      "Max reward so far:  -1542.0\n",
      "-0.0021291212\n",
      "==========================================\n",
      "Episode:  62\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2460.6825396825398\n",
      "Max reward so far:  -1542.0\n",
      "0.0073515326\n",
      "==========================================\n",
      "Episode:  63\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2461.296875\n",
      "Max reward so far:  -1542.0\n",
      "-0.014339904\n",
      "==========================================\n",
      "Episode:  64\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2461.892307692308\n",
      "Max reward so far:  -1542.0\n",
      "-0.0035686586\n",
      "==========================================\n",
      "Episode:  65\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2462.469696969697\n",
      "Max reward so far:  -1542.0\n",
      "-0.0051891734\n",
      "==========================================\n",
      "Episode:  66\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2463.0298507462685\n",
      "Max reward so far:  -1542.0\n",
      "-0.017274255\n",
      "==========================================\n",
      "Episode:  67\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2463.573529411765\n",
      "Max reward so far:  -1542.0\n",
      "0.0033076075\n",
      "==========================================\n",
      "Episode:  68\n",
      "Reward:  -1594.0\n",
      "Mean Reward -2450.9710144927535\n",
      "Max reward so far:  -1542.0\n",
      "0.011972832\n",
      "==========================================\n",
      "Episode:  69\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2451.6714285714284\n",
      "Max reward so far:  -1542.0\n",
      "-0.0036722866\n",
      "==========================================\n",
      "Episode:  70\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2452.3521126760565\n",
      "Max reward so far:  -1542.0\n",
      "-0.015384023\n",
      "==========================================\n",
      "Episode:  71\n",
      "Reward:  -2500.0\n",
      "Mean Reward -2453.0138888888887\n",
      "Max reward so far:  -1542.0\n",
      "-0.021710243\n",
      "==========================================\n",
      "Episode:  72\n",
      "Reward:  -1446.0\n",
      "Mean Reward -2439.219178082192\n",
      "Max reward so far:  -1446.0\n",
      "0.003165859\n",
      "==========================================\n",
      "Episode:  73\n",
      "Reward:  -2201.0\n",
      "Mean Reward -2436.0\n",
      "Max reward so far:  -1446.0\n",
      "0.043025453\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-275a3b85855a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# Choose action a, remember WE'RE NOT IN A DETERMINISTIC ENVIRONMENT, WE'RE OUTPUT PROBABILITIES.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0maction_probability_distribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_distribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probability_distribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_probability_distribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# select action w.r.t the actions prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "allRewards = []\n",
    "total_rewards = 0\n",
    "maximumRewardRecorded = 0\n",
    "episode = 0\n",
    "episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "losses = []\n",
    "avgloss = 1\n",
    "with g.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "env.reset()\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # each episode is one trajectory\n",
    "    for episode in range(max_episodes):\n",
    "        episode_rewards_sum = 0\n",
    "        state = env.reset()\n",
    "        env.render()\n",
    "        step=1\n",
    "        while True:\n",
    "            step+=1\n",
    "            if(episode % 100 == 0 and episode != 0):\n",
    "                env.render()\n",
    "            # Choose action a, remember WE'RE NOT IN A DETERMINISTIC ENVIRONMENT, WE'RE OUTPUT PROBABILITIES.\n",
    "            action_probability_distribution = sess.run(action_distribution, feed_dict={input_: state.reshape([1,2])})\n",
    "            \n",
    "            action = np.random.choice(range(action_probability_distribution.shape[1]), p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
    "\n",
    "            # Perform a\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            if(step>2500):\n",
    "                done=True\n",
    "            # Store s, a, r\n",
    "            episode_states.append(state)\n",
    "                        \n",
    "            # For actions because we output only one (the index) we need 2 (1 is for the action taken)\n",
    "            # We need [0., 1.] (if we take right) not just the index\n",
    "            action_ = np.zeros(action_size)\n",
    "            action_[action] = 1\n",
    "            \n",
    "            episode_actions.append(action_)\n",
    "            \n",
    "            episode_rewards.append(reward)\n",
    "            if done:\n",
    "                # Calculate sum reward\n",
    "                episode_rewards_sum = np.sum(episode_rewards)\n",
    "                \n",
    "                allRewards.append(episode_rewards_sum)\n",
    "                \n",
    "                total_rewards = np.sum(allRewards)\n",
    "                \n",
    "                # Mean reward\n",
    "                mean_reward = np.divide(total_rewards, episode+1)\n",
    "                \n",
    "                \n",
    "                maximumRewardRecorded = np.amax(allRewards)\n",
    "                \n",
    "                print(\"==========================================\")\n",
    "                print(\"Episode: \", episode)\n",
    "                print(\"Reward: \", episode_rewards_sum)\n",
    "                print(\"Mean Reward\", mean_reward)\n",
    "                print(\"Max reward so far: \", maximumRewardRecorded)\n",
    "                \n",
    "                # Calculate discounted reward\n",
    "                discounted_episode_rewards = discount_and_normalize_rewards(episode_rewards)\n",
    "                                \n",
    "                # Feedforward, gradient and backpropagation\n",
    "                loss_, train_, fc3_, actions_ = sess.run([loss, train_opt, fc3, action_distribution], feed_dict={input_: np.vstack(np.array(episode_states)),\n",
    "                                                                 actions: np.vstack(np.array(episode_actions)),\n",
    "                                                                 discounted_episode_rewards_: discounted_episode_rewards \n",
    "                                                                })\n",
    "                losses.append(loss_)\n",
    "                avgloss = np.mean(losses)\n",
    "                print(loss_)\n",
    "                #print(actions_)\n",
    " \n",
    "                                                                 \n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={input_: np.vstack(np.array(episode_states)),\n",
    "                                                                 actions: np.vstack(np.array(episode_actions)),\n",
    "                                                                 discounted_episode_rewards_: discounted_episode_rewards,\n",
    "                                                                    mean_reward_: mean_reward,\n",
    "                                                                avg_loss: avgloss\n",
    "                                                                })\n",
    "                \n",
    "               \n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "                \n",
    "            \n",
    "                \n",
    "                # Reset the transition stores\n",
    "                episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "                \n",
    "                break\n",
    "            \n",
    "            state = new_state\n",
    "        \n",
    "        # Save Model\n",
    "        if episode % 100 == 0:\n",
    "            saver.save(sess, \"./models/model.ckpt\")\n",
    "            print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
